import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import joblib

# ========== 1. è³‡æ–™è®€å–èˆ‡é è™•ç† ==========

# è¼‰å…¥æª”æ¡ˆï¼ˆç¬¬3å€‹æª”æ¡ˆï¼‰
dataset = pd.read_csv('data/' + os.listdir('data')[2])
df = pd.DataFrame(dataset)

# å¡«è£œç¼ºå¤±å€¼ï¼ˆåªé‡å°æœ‰ç”¨æ¬„ä½ï¼‰
cols = ['Temperature', 'EC', 'PH', 'Salinity', 'DO']
imputer = SimpleImputer(strategy='mean')
data = pd.DataFrame(imputer.fit_transform(df[cols]), columns=cols)

# å»ºç«‹è¡ç”Ÿç‰¹å¾µ
data['Temp^2'] = data['Temperature'] ** 2
data['EC/PH'] = data['EC'] / (data['PH'] + 1e-5)
data['EC*Temp'] = data['EC'] * data['Temperature']

# ç‰¹å¾µèˆ‡ç›®æ¨™åˆ†é›¢
X = data.drop(columns=['DO'])
y = data['DO']

# ========== 2. åˆ‡åˆ†è³‡æ–™é›† ==========

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ========== 3. éš¨æ©Ÿæ£®æ— + åƒæ•¸æœå°‹ + KFold ==========

rf = RandomForestRegressor(random_state=42)

param_grid = {
    'n_estimators': [150],       # æå‡æ¨¡å‹ç©©å®šæ€§
    'max_depth': [20],            # æ”¾å¯¬æ·±åº¦ï¼Œå­¸åˆ°æ›´å¤šè¤‡é›œé—œä¿‚
    'min_samples_split': [5],      # æ¸›å°‘åˆ†è£‚é–€æª»ï¼Œè®“æ¨¹æ›´æ´»èº
    'min_samples_leaf': [2]        # è‘‰ç¯€é»æ¨£æœ¬è®Šå°‘ï¼Œèƒ½æ•æ‰ç´°ç¯€
}

cv = KFold(n_splits=5, shuffle=True, random_state=42)

grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=cv,
    scoring='r2',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)
best_rf = grid_search.best_estimator_

print("æœ€ä½³åƒæ•¸ï¼š", grid_search.best_params_)

# ========== 4. é æ¸¬èˆ‡è©•ä¼° ==========

def evaluate_model(model, X, y, dataset_name="è³‡æ–™é›†"):
    y_pred = model.predict(X)
    r2 = r2_score(y, y_pred)
    mae = mean_absolute_error(y, y_pred)
    rmse = mean_squared_error(y, y_pred) ** 0.5
    print(f"\n=== {dataset_name} è¡¨ç¾ ===")
    print("RÂ²:", round(r2, 4))
    print("MAE:", round(mae, 4))
    print("RMSE:", round(rmse, 4))
    return r2, mae, rmse

evaluate_model(best_rf, X_train, y_train, "è¨“ç·´é›†")
evaluate_model(best_rf, X_test, y_test, "æ¸¬è©¦é›†")

# ========== 5. ç‰¹å¾µé‡è¦æ€§åˆ—å° ==========

feature_importances = pd.Series(
    best_rf.feature_importances_, index=X.columns
).sort_values(ascending=False)

print("\nğŸ“Š ç‰¹å¾µé‡è¦æ€§ï¼ˆç”±é«˜åˆ°ä½ï¼‰ï¼š")
print(feature_importances)

# ========== 6. æ¨¡å‹å„²å­˜ ==========

joblib.dump(best_rf, 'PredictDO.pkl')
print("\nâœ… æ¨¡å‹å·²å„²å­˜ç‚º PredictDO.pkl")

# ========== é¡¯ç¤ºç›¸é—œä¿‚æ•¸ ==========

print("\nğŸ“ˆ å„ç‰¹å¾µèˆ‡ DO çš„ç›¸é—œä¿‚æ•¸ï¼ˆPearson rï¼‰:")
corr_matrix = data.corr()
print(corr_matrix['DO'].sort_values(ascending=False))